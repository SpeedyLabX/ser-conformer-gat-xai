{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a227da56",
   "metadata": {},
   "source": [
    "# RoBERTa full fine-tune (Kaggle)\n",
    "\n",
    "This notebook runs a full RoBERTa-base fine-tune for the TEXT_BRANCH on Kaggle.\n",
    "\n",
    "Usage notes:\n",
    "- Upload `data/iemocap_manifest.jsonl` as a Kaggle Dataset and add it to the Notebook (recommended).\n",
    "- Use GPU Accelerator (T4 or better).\n",
    "- Adjust `--batch_size`, `--accumulation_steps`, and `--max_length` depending on GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -U transformers accelerate datasets evaluate sentencepiece sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, transformers, datasets, numpy as np\n",
    "\n",
    "print(\"torch:\", torch.__version__, \"cuda:\", torch.cuda.is_available())\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"datasets:\", datasets.__version__)\n",
    "print(\"numpy:\", np.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/SpeedyLabX/ser-conformer-gat-xai.git\n",
    "%cd ser-conformer-gat-xai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir -p data\n",
    "!cp /kaggle/input/slx02-ser-dataset/iemocap_manifest.jsonl data/iemocap_manifest.jsonl\n",
    "!ls -l data || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run RoBERTa fine-tune (recommended settings for Kaggle P100)\n",
    "!python scripts/finetune_roberta.py --manifest data/iemocap_manifest.jsonl --backbone roberta-base --batch_size 8 --accumulation_steps 4 --epochs 100 --max_length 128 --num_class 7 --out_dir /kaggle/working/artifacts/roberta_kaggle --fp16 --gradient_checkpointing --load_best_model_at_end --save_total_limit 5 --evaluation_strategy epoch --early_stopping_patience 5 --lr 1e-5 --use_tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a4d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python scripts/evaluate_text.py --manifest data/iemocap_manifest.jsonl --checkpoint artifacts/roberta_kaggle/pytorch_model.bin --backbone roberta-base --proj_dim 256 --out_dir artifacts/roberta_kaggle_eval --save_confusion_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14243e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List and zip artifacts for download\n",
    "!ls -la /kaggle/working/artifacts/roberta_kaggle || true\n",
    "!zip -r /kaggle/working/roberta_kaggle_artifacts.zip /kaggle/working/artifacts/roberta_kaggle || true"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
