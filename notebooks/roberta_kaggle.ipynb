{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a227da56",
   "metadata": {},
   "source": [
    "# RoBERTa full fine-tune (Kaggle)\n",
    "\n",
    "This notebook runs a full RoBERTa-base fine-tune for the TEXT_BRANCH on Kaggle.\n",
    "\n",
    "Usage notes:\n",
    "- Upload `data/iemocap_manifest.jsonl` as a Kaggle Dataset and add it to the Notebook (recommended).\n",
    "- Use GPU Accelerator (T4 or better).\n",
    "- Adjust `--batch_size`, `--accumulation_steps`, and `--max_length` depending on GPU memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Optional: install pinned Python packages (Kaggle often has CUDA-enabled PyTorch preinstalled)\n",
    "!pip install --quiet numpy==1.24.4 transformers==4.57.1 datasets sentence-transformers accelerate==0.20.3 huggingface-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Clone the repository and change to repo root\n",
    "!git clone https://github.com/SpeedyLabX/ser-conformer-gat-xai.git\n",
    "%cd ser-conformer-gat-xai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Copy your Kaggle Dataset manifest into the repo (example).\n",
    "# If you added the dataset to the session, the file will be under /kaggle/input/<dataset-name>/...\n",
    "# Replace <dataset-name> and path as appropriate.\n",
    "# !cp /kaggle/input/<dataset-name>/iemocap_manifest.jsonl data/iemocap_manifest.jsonl\n",
    "!ls -l data || true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Run the finetune (adjust arguments to match GPU).\n",
    "# Recommended: --batch_size 8 --accumulation_steps 4 --epochs 3 --max_length 128 --fp16\n",
    "!python scripts/finetune_roberta.py --manifest data/iemocap_manifest.jsonl --backbone roberta-base --batch_size 8 --accumulation_steps 4 --epochs 3 --max_length 128 --num_class 7 --out_dir /kaggle/working/artifacts/roberta_kaggle --fp16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) List artifacts and zip them for download\n",
    "!ls -la /kaggle/working/artifacts/roberta_kaggle || true\n",
    "!zip -r /kaggle/working/roberta_kaggle_artifacts.zip /kaggle/working/artifacts/roberta_kaggle || true"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
