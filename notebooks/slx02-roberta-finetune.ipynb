{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a227da56",
   "metadata": {},
   "source": [
    "# SLX02 Text Branch: RoBERTa Fine-tuning (Kaggle)\n",
    "\n",
    "**Project**: SLX02 - Conformer-GAT Fusion for Speech Emotion Recognition  \n",
    "**Target**: 68-71% WA on IEMOCAP 4-class (text-only)  \n",
    "**Benchmark**: RobinNet (2024) achieves 71.1% WA with RoBERTa\n",
    "\n",
    "## Key Improvements (Based on Literature Review)\n",
    "1. **LOSO Protocol** - Leave-One-Session-Out for speaker independence\n",
    "2. **4-Class Mapping** - Standard: neu, hap+exc, ang+fru, sad\n",
    "3. **Attention Pooling** - Better than mean/cls for SER\n",
    "4. **Label Smoothing** - Prevents overconfidence\n",
    "5. **Class Weights** - Handles imbalanced data\n",
    "6. **Reports WA & UA** - Required for literature comparison\n",
    "\n",
    "## Usage\n",
    "- Upload `data/iemocap_manifest.jsonl` as Kaggle Dataset\n",
    "- Use GPU Accelerator (T4 x2 or P100)\n",
    "- Runtime: ~30-60 min per fold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7f89c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install -q -U transformers==4.44.0 accelerate>=1.0.0 datasets>=2.14 scikit-learn>=1.3.0 tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722cd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check environment\n",
    "import torch\n",
    "import transformers\n",
    "import numpy as np\n",
    "import json\n",
    "import re\n",
    "import random\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "print(f\"Transformers: {transformers.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36e1238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "class Config:\n",
    "    # Data\n",
    "    MANIFEST_PATH = \"/kaggle/input/slx02-ser-dataset/iemocap_manifest.jsonl\"\n",
    "    OUT_DIR = \"/kaggle/working/text_branch_results\"\n",
    "    \n",
    "    # Model\n",
    "    BACKBONE = \"roberta-base\"  # Best for text-only SER (RobinNet benchmark)\n",
    "    MAX_LENGTH = 128\n",
    "    POOLING = \"attention\"  # \"attention\", \"cls\", or \"mean\"\n",
    "    \n",
    "    # Training\n",
    "    EPOCHS = 30\n",
    "    BATCH_SIZE = 16  # Adjust based on GPU memory\n",
    "    LR = 2e-5  # For classifier; encoder uses 0.1x\n",
    "    WEIGHT_DECAY = 0.01\n",
    "    WARMUP_RATIO = 0.1\n",
    "    GRAD_CLIP = 1.0\n",
    "    \n",
    "    # Regularization\n",
    "    DROPOUT = 0.3\n",
    "    LABEL_SMOOTHING = 0.1\n",
    "    USE_CLASS_WEIGHTS = True\n",
    "    \n",
    "    # Early stopping\n",
    "    PATIENCE = 5\n",
    "    \n",
    "    # Evaluation\n",
    "    NUM_CLASSES = 4  # Standard 4-class IEMOCAP\n",
    "    \n",
    "    # Reproducibility\n",
    "    SEED = 42\n",
    "\n",
    "config = Config()\n",
    "Path(config.OUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {config.OUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71c0c825",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4-Class Label Mapping (Standard IEMOCAP)\n",
    "# =============================================================================\n",
    "# From literature: neu(0), hap+exc(1), ang+fru(2), sad(3)\n",
    "\n",
    "LABEL_4CLASS_MAP = {\n",
    "    \"neu\": 0, \"neutral\": 0,\n",
    "    \"hap\": 1, \"happy\": 1, \"exc\": 1, \"excited\": 1,\n",
    "    \"ang\": 2, \"angry\": 2, \"fru\": 2, \"frustration\": 2,\n",
    "    \"sad\": 3, \"sadness\": 3,\n",
    "}\n",
    "\n",
    "LABEL_NAMES = [\"neutral\", \"happy\", \"angry\", \"sad\"]\n",
    "\n",
    "def canonicalize_label(label) -> int:\n",
    "    \"\"\"Map label to 4-class index. Returns -1 for invalid.\"\"\"\n",
    "    if label is None:\n",
    "        return -1\n",
    "    if isinstance(label, int):\n",
    "        # 6-class to 4-class: 0->0, 1->1, 2->2, 3->3, 4->1(exc), 5->2(fru)\n",
    "        map_6_to_4 = {0: 0, 1: 1, 2: 2, 3: 3, 4: 1, 5: 2}\n",
    "        return map_6_to_4.get(label, -1)\n",
    "    key = str(label).lower().strip()\n",
    "    return LABEL_4CLASS_MAP.get(key, -1)\n",
    "\n",
    "def get_session_id(record: Dict) -> Optional[int]:\n",
    "    \"\"\"Extract session number from record.\"\"\"\n",
    "    s = record.get(\"session\")\n",
    "    if s is None:\n",
    "        return None\n",
    "    if isinstance(s, int):\n",
    "        return s\n",
    "    m = re.search(r\"(\\d+)\", str(s))\n",
    "    return int(m.group(1)) if m else None\n",
    "\n",
    "print(\"Label mapping defined:\")\n",
    "print(f\"  Classes: {LABEL_NAMES}\")\n",
    "print(f\"  Mapping: neu→0, hap+exc→1, ang+fru→2, sad→3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863bf995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Load and Prepare Data\n",
    "# =============================================================================\n",
    "\n",
    "def read_manifest(path: str) -> List[Dict]:\n",
    "    \"\"\"Read JSONL manifest file.\"\"\"\n",
    "    records = []\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line:\n",
    "                records.append(json.loads(line))\n",
    "    return records\n",
    "\n",
    "def filter_valid_records(records: List[Dict]) -> List[Dict]:\n",
    "    \"\"\"Keep only records with valid 4-class labels.\"\"\"\n",
    "    valid = []\n",
    "    for r in records:\n",
    "        label = canonicalize_label(r.get(\"label\"))\n",
    "        if label >= 0:\n",
    "            valid.append(r)\n",
    "    return valid\n",
    "\n",
    "def loso_split(records: List[Dict], test_session: int) -> Tuple[List[Dict], List[Dict]]:\n",
    "    \"\"\"Leave-One-Session-Out split.\"\"\"\n",
    "    train, test = [], []\n",
    "    for r in records:\n",
    "        sid = get_session_id(r)\n",
    "        if sid == test_session:\n",
    "            test.append(r)\n",
    "        else:\n",
    "            train.append(r)\n",
    "    return train, test\n",
    "\n",
    "def get_class_distribution(records: List[Dict]) -> Dict[int, int]:\n",
    "    \"\"\"Get class counts.\"\"\"\n",
    "    dist = defaultdict(int)\n",
    "    for r in records:\n",
    "        label = canonicalize_label(r.get(\"label\"))\n",
    "        if label >= 0:\n",
    "            dist[label] += 1\n",
    "    return dict(dist)\n",
    "\n",
    "# Load data\n",
    "print(f\"Loading manifest from: {config.MANIFEST_PATH}\")\n",
    "all_records = read_manifest(config.MANIFEST_PATH)\n",
    "all_records = filter_valid_records(all_records)\n",
    "\n",
    "print(f\"\\nTotal valid samples (4-class): {len(all_records)}\")\n",
    "dist = get_class_distribution(all_records)\n",
    "print(f\"Class distribution:\")\n",
    "for i, name in enumerate(LABEL_NAMES):\n",
    "    count = dist.get(i, 0)\n",
    "    pct = count / len(all_records) * 100\n",
    "    print(f\"  {name}: {count} ({pct:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d6aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Model Components\n",
    "# =============================================================================\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "\n",
    "class AttentionPooling(nn.Module):\n",
    "    \"\"\"Attention-based pooling (better than mean/cls for SER).\"\"\"\n",
    "    \n",
    "    def __init__(self, hidden_dim: int, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(hidden_dim, hidden_dim // 4),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim // 4, 1),\n",
    "        )\n",
    "    \n",
    "    def forward(self, tokens: torch.Tensor, mask: torch.Tensor = None) -> torch.Tensor:\n",
    "        weights = self.attention(tokens).squeeze(-1)  # (B, T)\n",
    "        if mask is not None:\n",
    "            weights = weights.masked_fill(mask == 0, float(\"-inf\"))\n",
    "        weights = F.softmax(weights, dim=-1)\n",
    "        pooled = torch.bmm(weights.unsqueeze(1), tokens).squeeze(1)\n",
    "        return pooled\n",
    "\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    \"\"\"RoBERTa + Attention Pooling + Classification Head.\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        backbone: str = \"roberta-base\",\n",
    "        num_classes: int = 4,\n",
    "        dropout: float = 0.3,\n",
    "        pooling: str = \"attention\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.encoder = AutoModel.from_pretrained(backbone)\n",
    "        hidden_size = self.encoder.config.hidden_size\n",
    "        \n",
    "        self.pooling_type = pooling\n",
    "        if pooling == \"attention\":\n",
    "            self.pooler = AttentionPooling(hidden_size, dropout)\n",
    "        else:\n",
    "            self.pooler = None\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, hidden_size // 2),\n",
    "            nn.LayerNorm(hidden_size // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size // 2, num_classes),\n",
    "        )\n",
    "        \n",
    "        self.hidden_size = hidden_size\n",
    "    \n",
    "    def forward(self, input_ids, attention_mask, return_features=False):\n",
    "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        tokens = outputs.last_hidden_state\n",
    "        \n",
    "        if self.pooling_type == \"attention\" and self.pooler:\n",
    "            pooled = self.pooler(tokens, attention_mask)\n",
    "        elif self.pooling_type == \"cls\":\n",
    "            pooled = tokens[:, 0, :]\n",
    "        else:  # mean\n",
    "            mask = attention_mask.unsqueeze(-1)\n",
    "            pooled = (tokens * mask).sum(1) / mask.sum(1).clamp(min=1e-8)\n",
    "        \n",
    "        logits = self.classifier(pooled)\n",
    "        \n",
    "        if return_features:\n",
    "            return {\"logits\": logits, \"features\": pooled}\n",
    "        return {\"logits\": logits}\n",
    "\n",
    "print(\"Model components defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd140aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Dataset and DataLoader\n",
    "# =============================================================================\n",
    "\n",
    "class IEMOCAPTextDataset(Dataset):\n",
    "    \"\"\"Text dataset for IEMOCAP with 4-class labels.\"\"\"\n",
    "    \n",
    "    def __init__(self, records: List[Dict], tokenizer, max_length: int = 128):\n",
    "        self.records = records\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        r = self.records[idx]\n",
    "        text = r.get(\"text\", \"\")\n",
    "        label = canonicalize_label(r.get(\"label\"))\n",
    "        \n",
    "        encoded = self.tokenizer(\n",
    "            text,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoded[\"input_ids\"].squeeze(0),\n",
    "            \"attention_mask\": encoded[\"attention_mask\"].squeeze(0),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long),\n",
    "        }\n",
    "\n",
    "\n",
    "class LabelSmoothingCE(nn.Module):\n",
    "    \"\"\"Cross entropy with label smoothing.\"\"\"\n",
    "    \n",
    "    def __init__(self, smoothing: float = 0.1, weight: torch.Tensor = None):\n",
    "        super().__init__()\n",
    "        self.smoothing = smoothing\n",
    "        self.weight = weight\n",
    "    \n",
    "    def forward(self, pred: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
    "        n_classes = pred.size(-1)\n",
    "        with torch.no_grad():\n",
    "            true_dist = torch.zeros_like(pred)\n",
    "            true_dist.fill_(self.smoothing / (n_classes - 1))\n",
    "            true_dist.scatter_(1, target.unsqueeze(1), 1.0 - self.smoothing)\n",
    "        \n",
    "        log_probs = F.log_softmax(pred, dim=-1)\n",
    "        loss = (-true_dist * log_probs).sum(dim=-1)\n",
    "        \n",
    "        if self.weight is not None:\n",
    "            weight = self.weight[target]\n",
    "            loss = loss * weight\n",
    "        \n",
    "        return loss.mean()\n",
    "\n",
    "print(\"Dataset and loss defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a4d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Training and Evaluation Functions\n",
    "# =============================================================================\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "def set_seed(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def compute_metrics(y_true: np.ndarray, y_pred: np.ndarray) -> Dict:\n",
    "    \"\"\"Compute WA, UA, F1, and confusion matrix.\"\"\"\n",
    "    mask = y_true >= 0\n",
    "    y_true, y_pred = y_true[mask], y_pred[mask]\n",
    "    \n",
    "    if len(y_true) == 0:\n",
    "        return {\"error\": \"No valid samples\"}\n",
    "    \n",
    "    return {\n",
    "        \"WA\": accuracy_score(y_true, y_pred),\n",
    "        \"UA\": recall_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"F1_macro\": f1_score(y_true, y_pred, average=\"macro\", zero_division=0),\n",
    "        \"confusion_matrix\": confusion_matrix(y_true, y_pred, labels=range(4)).tolist(),\n",
    "        \"n_samples\": len(y_true),\n",
    "    }\n",
    "\n",
    "def train_epoch(model, loader, optimizer, scheduler, loss_fn, device, grad_clip=1.0):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Training\", leave=False)\n",
    "    for batch in pbar:\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        mask = labels >= 0\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        output = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(output[\"logits\"][mask], labels[mask])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        if grad_clip > 0:\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item() * mask.sum().item()\n",
    "        all_preds.extend(output[\"logits\"].argmax(-1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    metrics[\"loss\"] = total_loss / len(all_labels) if all_labels else 0\n",
    "    return metrics\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, loss_fn, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    all_preds, all_labels = [], []\n",
    "    \n",
    "    for batch in tqdm(loader, desc=\"Evaluating\", leave=False):\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        \n",
    "        mask = labels >= 0\n",
    "        if mask.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        output = model(input_ids, attention_mask)\n",
    "        loss = loss_fn(output[\"logits\"][mask], labels[mask])\n",
    "        \n",
    "        total_loss += loss.item() * mask.sum().item()\n",
    "        all_preds.extend(output[\"logits\"].argmax(-1).cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    metrics = compute_metrics(np.array(all_labels), np.array(all_preds))\n",
    "    metrics[\"loss\"] = total_loss / len(all_labels) if all_labels else 0\n",
    "    return metrics\n",
    "\n",
    "print(\"Training functions defined ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e764170c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Train Single Fold\n",
    "# =============================================================================\n",
    "\n",
    "def train_fold(train_records, val_records, fold_id, config):\n",
    "    \"\"\"Train a single LOSO fold.\"\"\"\n",
    "    set_seed(config.SEED + fold_id)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FOLD {fold_id} | Train: {len(train_records)} | Val: {len(val_records)}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config.BACKBONE)\n",
    "    \n",
    "    # Datasets\n",
    "    train_ds = IEMOCAPTextDataset(train_records, tokenizer, config.MAX_LENGTH)\n",
    "    val_ds = IEMOCAPTextDataset(val_records, tokenizer, config.MAX_LENGTH)\n",
    "    \n",
    "    train_loader = DataLoader(train_ds, batch_size=config.BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "    \n",
    "    # Model\n",
    "    model = TextClassifier(\n",
    "        backbone=config.BACKBONE,\n",
    "        num_classes=config.NUM_CLASSES,\n",
    "        dropout=config.DROPOUT,\n",
    "        pooling=config.POOLING,\n",
    "    ).to(device)\n",
    "    \n",
    "    # Class weights\n",
    "    train_dist = get_class_distribution(train_records)\n",
    "    counts = np.array([train_dist.get(i, 1) for i in range(4)])\n",
    "    weights = 1.0 / (counts + 1e-6)\n",
    "    weights = weights / weights.sum() * 4\n",
    "    class_weights = torch.tensor(weights, dtype=torch.float32).to(device) if config.USE_CLASS_WEIGHTS else None\n",
    "    \n",
    "    # Loss\n",
    "    loss_fn = LabelSmoothingCE(config.LABEL_SMOOTHING, class_weights)\n",
    "    \n",
    "    # Optimizer with differential LR\n",
    "    encoder_params = list(model.encoder.parameters())\n",
    "    head_params = list(model.classifier.parameters())\n",
    "    if model.pooler:\n",
    "        head_params += list(model.pooler.parameters())\n",
    "    \n",
    "    optimizer = AdamW([\n",
    "        {\"params\": encoder_params, \"lr\": config.LR * 0.1},\n",
    "        {\"params\": head_params, \"lr\": config.LR},\n",
    "    ], weight_decay=config.WEIGHT_DECAY)\n",
    "    \n",
    "    # Scheduler\n",
    "    total_steps = len(train_loader) * config.EPOCHS\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=[config.LR * 0.1, config.LR],\n",
    "        total_steps=total_steps,\n",
    "        pct_start=config.WARMUP_RATIO,\n",
    "        anneal_strategy=\"cos\",\n",
    "    )\n",
    "    \n",
    "    # Training loop\n",
    "    best_wa = 0\n",
    "    best_epoch = 0\n",
    "    patience_counter = 0\n",
    "    history = []\n",
    "    \n",
    "    for epoch in range(config.EPOCHS):\n",
    "        train_metrics = train_epoch(model, train_loader, optimizer, scheduler, loss_fn, device, config.GRAD_CLIP)\n",
    "        val_metrics = evaluate(model, val_loader, loss_fn, device)\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d}/{config.EPOCHS} | \"\n",
    "              f\"Train Loss: {train_metrics['loss']:.4f} WA: {train_metrics['WA']*100:.2f}% | \"\n",
    "              f\"Val Loss: {val_metrics['loss']:.4f} WA: {val_metrics['WA']*100:.2f}% UA: {val_metrics['UA']*100:.2f}%\")\n",
    "        \n",
    "        history.append({\n",
    "            \"epoch\": epoch + 1,\n",
    "            \"train_loss\": train_metrics[\"loss\"],\n",
    "            \"train_WA\": train_metrics[\"WA\"],\n",
    "            \"val_loss\": val_metrics[\"loss\"],\n",
    "            \"val_WA\": val_metrics[\"WA\"],\n",
    "            \"val_UA\": val_metrics[\"UA\"],\n",
    "        })\n",
    "        \n",
    "        # Save best\n",
    "        if val_metrics[\"WA\"] > best_wa:\n",
    "            best_wa = val_metrics[\"WA\"]\n",
    "            best_ua = val_metrics[\"UA\"]\n",
    "            best_epoch = epoch + 1\n",
    "            patience_counter = 0\n",
    "            \n",
    "            torch.save({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"val_WA\": best_wa,\n",
    "                \"val_UA\": best_ua,\n",
    "            }, f\"{config.OUT_DIR}/fold{fold_id}_best.pt\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= config.PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n✓ Fold {fold_id} Best: WA={best_wa*100:.2f}% UA={best_ua*100:.2f}% (Epoch {best_epoch})\")\n",
    "    \n",
    "    # Load best and get final metrics\n",
    "    ckpt = torch.load(f\"{config.OUT_DIR}/fold{fold_id}_best.pt\")\n",
    "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
    "    final_metrics = evaluate(model, val_loader, loss_fn, device)\n",
    "    final_metrics[\"best_epoch\"] = best_epoch\n",
    "    final_metrics[\"history\"] = history\n",
    "    \n",
    "    return final_metrics, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece8a308",
   "metadata": {},
   "source": [
    "## Run Training\n",
    "\n",
    "Choose one of:\n",
    "1. **Single Fold** - Quick test on one session (faster)\n",
    "2. **Full LOSO** - Complete 5-fold cross-validation (for final results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0dd2a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Option 1: Single Fold (Test on Session 5)\n",
    "# =============================================================================\n",
    "# Uncomment to run single fold for quick testing\n",
    "\n",
    "TEST_SESSION = 5\n",
    "\n",
    "train_records, val_records = loso_split(all_records, TEST_SESSION)\n",
    "\n",
    "# Split train into train/val for early stopping (90/10)\n",
    "np.random.seed(config.SEED)\n",
    "indices = np.random.permutation(len(train_records))\n",
    "val_size = int(len(train_records) * 0.1)\n",
    "train_subset = [train_records[i] for i in indices[val_size:]]\n",
    "val_subset = [train_records[i] for i in indices[:val_size]]\n",
    "\n",
    "print(f\"Session {TEST_SESSION} held out as test set\")\n",
    "print(f\"Training: {len(train_subset)} | Validation: {len(val_subset)} | Test: {len(val_records)}\")\n",
    "\n",
    "fold_metrics, trained_model = train_fold(train_subset, val_subset, TEST_SESSION, config)\n",
    "\n",
    "# Also evaluate on held-out test session\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.BACKBONE)\n",
    "test_ds = IEMOCAPTextDataset(val_records, tokenizer, config.MAX_LENGTH)\n",
    "test_loader = DataLoader(test_ds, batch_size=config.BATCH_SIZE, shuffle=False, num_workers=2)\n",
    "\n",
    "# Compute test metrics\n",
    "trained_model.eval()\n",
    "loss_fn = LabelSmoothingCE(config.LABEL_SMOOTHING)\n",
    "test_metrics = evaluate(trained_model, test_loader, loss_fn, torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TEST SET RESULTS (Session {TEST_SESSION})\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"WA: {test_metrics['WA']*100:.2f}%\")\n",
    "print(f\"UA: {test_metrics['UA']*100:.2f}%\")\n",
    "print(f\"F1 (macro): {test_metrics['F1_macro']*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b03885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Option 2: Full LOSO Cross-Validation (5 folds)\n",
    "# =============================================================================\n",
    "# Uncomment below to run full LOSO (takes longer but gives proper results)\n",
    "\n",
    "\"\"\"\n",
    "all_fold_results = []\n",
    "\n",
    "for test_session in range(1, 6):\n",
    "    print(f\"\\n{'#'*60}\")\n",
    "    print(f\"# FOLD {test_session}: Test on Session {test_session}\")\n",
    "    print(f\"{'#'*60}\")\n",
    "    \n",
    "    train_recs, test_recs = loso_split(all_records, test_session)\n",
    "    \n",
    "    # Split train into train/val\n",
    "    np.random.seed(config.SEED)\n",
    "    indices = np.random.permutation(len(train_recs))\n",
    "    val_size = int(len(train_recs) * 0.1)\n",
    "    train_subset = [train_recs[i] for i in indices[val_size:]]\n",
    "    val_subset = [train_recs[i] for i in indices[:val_size]]\n",
    "    \n",
    "    metrics, _ = train_fold(train_subset, val_subset, test_session, config)\n",
    "    metrics[\"test_session\"] = test_session\n",
    "    all_fold_results.append(metrics)\n",
    "\n",
    "# Aggregate\n",
    "wa_scores = [r[\"WA\"] for r in all_fold_results]\n",
    "ua_scores = [r[\"UA\"] for r in all_fold_results]\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"LOSO CROSS-VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Average WA: {np.mean(wa_scores)*100:.2f}% ± {np.std(wa_scores)*100:.2f}%\")\n",
    "print(f\"Average UA: {np.mean(ua_scores)*100:.2f}% ± {np.std(ua_scores)*100:.2f}%\")\n",
    "print(f\"Per-fold WA: {[f'{wa*100:.2f}%' for wa in wa_scores]}\")\n",
    "\"\"\"\n",
    "print(\"Full LOSO code is commented out. Uncomment to run all 5 folds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258d7cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Visualize Confusion Matrix\n",
    "# =============================================================================\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, labels, title=\"Confusion Matrix\"):\n",
    "    fig, ax = plt.subplots(figsize=(8, 6))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    \n",
    "    ax.set(xticks=np.arange(len(labels)),\n",
    "           yticks=np.arange(len(labels)),\n",
    "           xticklabels=labels, yticklabels=labels,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "    \n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\", rotation_mode=\"anchor\")\n",
    "    \n",
    "    # Text annotations\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(len(labels)):\n",
    "        for j in range(len(labels)):\n",
    "            ax.text(j, i, format(cm[i, j], 'd'),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot test confusion matrix\n",
    "if 'test_metrics' in dir() and 'confusion_matrix' in test_metrics:\n",
    "    cm = np.array(test_metrics['confusion_matrix'])\n",
    "    fig = plot_confusion_matrix(cm, LABEL_NAMES, f\"Session {TEST_SESSION} Test Set\")\n",
    "    plt.savefig(f\"{config.OUT_DIR}/confusion_matrix_fold{TEST_SESSION}.png\", dpi=150, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Per-class accuracy\n",
    "    print(\"\\nPer-class Recall:\")\n",
    "    for i, name in enumerate(LABEL_NAMES):\n",
    "        recall = cm[i, i] / cm[i].sum() if cm[i].sum() > 0 else 0\n",
    "        print(f\"  {name}: {recall*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369c984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Save Results\n",
    "# =============================================================================\n",
    "\n",
    "results = {\n",
    "    \"method\": \"RoBERTa + Attention Pooling\",\n",
    "    \"backbone\": config.BACKBONE,\n",
    "    \"pooling\": config.POOLING,\n",
    "    \"dataset\": \"IEMOCAP 4-class\",\n",
    "    \"protocol\": \"LOSO (single fold)\" if 'TEST_SESSION' in dir() else \"LOSO 5-fold\",\n",
    "    \"test_session\": TEST_SESSION if 'TEST_SESSION' in dir() else \"all\",\n",
    "    \"test_WA\": test_metrics[\"WA\"] if 'test_metrics' in dir() else None,\n",
    "    \"test_UA\": test_metrics[\"UA\"] if 'test_metrics' in dir() else None,\n",
    "    \"test_F1\": test_metrics[\"F1_macro\"] if 'test_metrics' in dir() else None,\n",
    "    \"config\": {\n",
    "        \"epochs\": config.EPOCHS,\n",
    "        \"batch_size\": config.BATCH_SIZE,\n",
    "        \"lr\": config.LR,\n",
    "        \"dropout\": config.DROPOUT,\n",
    "        \"label_smoothing\": config.LABEL_SMOOTHING,\n",
    "        \"max_length\": config.MAX_LENGTH,\n",
    "    },\n",
    "    \"timestamp\": datetime.now().isoformat(),\n",
    "}\n",
    "\n",
    "# Save JSON\n",
    "results_path = f\"{config.OUT_DIR}/results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "# Print benchmark comparison\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"BENCHMARK COMPARISON (Text-only on IEMOCAP 4-class)\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  RobinNet (2024):        71.1% WA, 70.6% UA\")\n",
    "print(f\"  TSIN (2022):            68.7% WA\")\n",
    "print(f\"  ISSA-BiGRU-MHA (2024):  66.1% WA, 66.5% UA\")\n",
    "print(f\"  ─────────────────────────────────────────\")\n",
    "if 'test_metrics' in dir():\n",
    "    print(f\"  Our result:             {test_metrics['WA']*100:.1f}% WA, {test_metrics['UA']*100:.1f}% UA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503ce133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Download Artifacts (for Kaggle)\n",
    "# =============================================================================\n",
    "import shutil\n",
    "\n",
    "# Create zip of all artifacts\n",
    "shutil.make_archive(\"/kaggle/working/text_branch_artifacts\", \"zip\", config.OUT_DIR)\n",
    "print(f\"Artifacts zipped to: /kaggle/working/text_branch_artifacts.zip\")\n",
    "\n",
    "# List saved files\n",
    "print(\"\\nSaved files:\")\n",
    "for f in Path(config.OUT_DIR).glob(\"*\"):\n",
    "    size = f.stat().st_size / 1024\n",
    "    print(f\"  {f.name}: {size:.1f} KB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
